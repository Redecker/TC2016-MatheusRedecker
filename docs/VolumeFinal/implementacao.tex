%!TEX root = volumeFinal.tex 

\chapter{\label{chap:impl}Implementação}

A implementação do algoritmo de AHTN dentro do MicroRTS foi feita em duas etapas.
Primeiro foi feita a modelagem do domínio e criado qual heurística define o valor de utilidade para os estados.
Na segunda etapa foi acoplado o JSHOP2 com o MicroRTS, e logo após foi feita a implementação do algoritmo.


\section{Modelagem do domínio}

Antes de realizar a implementação do algoritmo de AHTN, é preciso criar um domínio de planejamento. 
O domínio de planejamento serve para que o JSHOP2 consiga gerar os planos que serão usados pelo algoritmo.

Antes de iniciar a modelagem do domínio, é preciso definir qual estratégia o domínio vai seguir.
O primeiro domínio é pensado em uma situação de jogo, onde o jogador quer treinar uma unidade de ataque para destruir o seu adversário.
Sendo assim, o jogador precisar ter um \textit{worker}, para coletar recursos, um quartel para construir unidades de ataque, e uma base para conseguir guardar seus recursos e criar o seu \textit{worker}. 
Após ter um quartel, o jogador treina uma unidade de ataque, e manda ela atacar.
A unidade \textit{ranged} foi a unidade de ataque escolhida para ser treinada.
Apenas quando a unidade \textit{ranged} é destruída que outra unidade é criada.
Este domínio precisa que o jogo comece com uma base, pois a base é necessária para treinar \textit{workers} e para guardar os recursos coletados.  
O domínio foi pensado para ter apenas um \textit{worker} para realizar a coleta de recursos, e um quartel para treinar as unidades de ataque.

A modelagem do domínio começa pela descrição dos operadores.
Cada ação que pode ser executada dentro do jogo é descrita como um operador no domínio.
Por exemplo, as ações de construir um quartel, e coletar recursos, são transformadas nos operadores $!buildbarrack$ e $!getresource$, respectivamente.
Já os métodos servem para determinar ações de mais alto nível, e devem respeitar a estratégia definida.
Por exemplo, a estratégia define que apenas um quartel é construído, então o método deve ter uma precondição para verificar se não existe outro quartel no jogo, caso não tenha, ainda é preciso checar se há recursos suficientes para construir. 
Cada método pode chamar outros métodos.
Isso acontece pois pode ser necessário realizar outra ação antes de conseguir realizar a ação do método.
Por exemplo, é necessário um quartel para treinar uma unidade de ataque, quando o método de treinar unidade é chamado, ele verifica que não há um quartel, e ele chama o método de construir o quartel.

Para ganhar o jogo é preciso atacar o adversário, por essa razão o método utilizado como objetivo no problema de planejamento é o $ataqueranged$.
Esse método desencadeia todas as outras chamadas de método dependendo do estado do ambiente.
No Apêndice~\ref{ap:estra1} está a descrição deste domínio, que é chamado de domínio 1.

O domínio 1 treina apenas uma unidade de ataque, o que acaba limitando o poder de ataque da estratégia.
Para tentar solucionar esse problema, outro domínio foi modelado.
Esse domínio tem a mesma estratégia do domínio anterior, com a diferença de que mais de uma unidade de ataque é criada.
Assim, enquanto uma unidade está atacando, outra pode estar sendo treinada.
No Apêndice~\ref{ap:estra2} está a descrição deste domínio, que é chamado de domínio 2.


\section{Heurística}

O algoritmo de AHTN requer uma função de avaliação para determinar o valor de utilidade para cada estado do jogo.
Essa função utiliza uma heurística para determinar esse valor de utilidade.
A primeira heurística criada leva em consideração apenas as unidades do jogador.
Quanto maior o número de unidades disponíveis no ambiente melhor a função de avaliação do estado.
As unidades são ponderadas pelo custo de treinamento ou de construção.
Por exemplo, um quartel precisa de 5 unidades de recurso para ser construído, enquanto um \textit{worker} necessita só de 1 para ser treinado, por isso o quartel acaba tendo peso 5, e o \textit{worker} peso 1. 
Essa heurística tem um problema, pois o jogador adversário pode estar com o dobro de unidades que isso não é levado em consideração, e nesse cenário o jogador adversário estaria na frente. 

Para resolver o problema das unidades do inimigo não serem levadas em consideração.
A segunda heurística utiliza a mesma maneira de calcular da primeira heurística, mas também calcula esse valor para as unidades do adversário. 
Com os dois valores, o valor das suas unidades é subtraído do valor das unidades do adversário. 
Assim em um cenário onde o jogo está com mais tropas a favor, o valor de utilidade é positivo, e caso o contrario negativo.
Esta heurística foi a escolhida para ser utilizada na implementação.

Se um jogador não tem uma base, e não tem recursos para construir uma, o jogo está perdido, pois a base é necessário para armazenar os recursos. Nesse caso, as duas heurísticas retornam um valor negativo, indicando o fim do jogo. 

\section{Implementação}

\subsection{Ações do MicroRTS}

Antes de iniciar a implementação do algoritmo de AHTN, é preciso criar os métodos que executam as ações dentro do MicroRTS. 
A camada de abstração fornece as classes que são responsáveis por exercer essas funções, mas ainda assim é preciso informar qual unidade está chamando o método.
Para isso foram criados 6 métodos que utilizam as classes da camada.
Cada método fica responsável por apenas um ação dentro do jogo.
Os métodos realizam as seguintes ações: construir uma base, construir um quartel, treinar um \textit{worker}, um \textit{worker} coleta recursos, treinar unidade de ataque, e uma unidade de ataque procura unidades adversárias para atacar.

\subsection{Geração dos planos}

O algoritmo de AHTN decompõem todas as possibilidades de plano para um objetivo com determinada configuração do jogo.
Para gerar os planos foi utilizado o planejador JSHOP2.
O JSHOP2 gera duas classes java, uma representando o domínio, e outra representando o problema.
A classe que representa o domínio não precisa ser alterada, pois são sempre os mesmo operadores e métodos que são utilizados pelo planejador para gerar o plano.
Mas o problema muda a cada nova configuração do ambiente.
Nesse caso a classe do problema deve ser alterado para que represente um novo estado.
Essa mudança foi feita alterando a classe java originada pelo JSHOP2. 
A mudança consistem em gerar novos predicados a cada nova configuração.
Para isso foi preciso uma análise de como é guardado internamente cada um dos componentes do problema de planejamento na classe do JSHOP2.
A partir dessa análise houve o mapeamento do estado do jogo para os predicados do problema.
Uma classe chamada de $EstadoDoJogo$ foi criada para representar o estado em que o jogo está em determinado momento.
A classe contém informações das unidades que cada jogador tem em determinada configuração do jogo, podendo ser alterada caso alguma ação seja executada.
A cada novo estado é gerado um conjunto de predicados que representam o jogo, para que o planejador consiga gerar quais planos são possíveis de serem feitos.

\subsection{Algoritmo de AHTN}

Para que o MicroRTS reconheça uma IA, ela deve ter o método $\mathit{getAction}$ implementado.
Quando este método é invocado a técnica deve realizar a sua operação e retornar qual movimento deve ser executado.
Sendo assim, a classe que implementa o algoritmo de AHTN deve utilizar este método para retornar a ação encontrada.
O algoritmo de AHTN implementado seguiu o padrão do algoritmo apresentado na Seção~\ref{alg:ahtn}, mas com algumas alterações.

O JSHOP2 gera os planos apenas com os operadores utilizados para alcançar o objetivo.
Com isso, o plano gerado não tem a informação de quais métodos foram utilizados para chegar aos operadores.
Neste caso, o plano gerado contém os operadores até chegar ao objetivo.
No algoritmo de AHTN original o plano vai sendo decomposto através dos métodos, e só realiza a troca de perspectiva entre \textsc{Max} e \textsc{Min} quando há uma tarefa primitiva a ser executada.
No algoritmo de AHTN implementado há sempre a troca de perspectiva a cada rodada.
A troca de perspectiva é feita para cada plano que é gerado a partir do estado do jogo.
O Algoritmo~\ref{alg:meuahtn} ilustra o pseudo código do algoritmo de AHTN implementado.

O algoritmo recebe um estado, representando qual a configuração do ambiente, um plano para a perspectiva \textsc{Max}, um plano para a perspectiva \textsc{Min}, e uma profundidade.
A linha~\ref{alg:meuahtn:terminal} é responsável por verificar se o estado é uma configuração de final de jogo, ou ainda se a profundidade configurada não chegou ao fim.
Caso uma das duas coisas aconteça, o algoritmo retorna o plano de cada perspectiva, e também a função de avaliação para a configuração em que o jogo se encontra.

O algoritmo avança caso não haja uma configuração de final de jogo.
Na linha~\ref{alg:meuahtn:nexaction} o algoritmo aplica a próxima ação primitiva do plano no estado.
Neste ponto, o jogador, na perspectiva do \textsc{Max}, realiza uma ação no estado.
Após alterar o estado, o algoritmo gera todos os planos para \textsc{Min} a partir do novo estado, e realiza a chamada para o método $\mathit{AHTNMin}$.
O método de $\mathit{AHTNMin}$ realiza as mesmas operações que o método de $\mathit{AHTNMax}$ mas com a perspectiva de \textsc{Min}.
Seguindo, na linha~\ref{alg:meuahtn:avali}, o algoritmo verifica qual dos planos obteve a melhor função de avaliação. 
Com isso, o algoritmo decide qual a melhor opção de plano.
O melhor plano é retornado na linha~\ref{alg:meuahtn:retorno}.


\begin{algorithm}[ht]
	\caption{Pseudo código do algoritmo de AHTN implementado.}
	\label{alg:meuahtn}
 	\begin{algorithmic}[1]		
 		\Function {ATHNMax}{$estado, planoMax, planoMin, deph$}
	 		\If {$terminal(estado) \vee d \leq 0$} \label{alg:meuahtn:terminal}
		 		\State	\Return $(planoMax, planoMin, avaliacao(estado))$
	 		\EndIf
	 		\State {$nextAction(planoMax)$} \label{alg:meuahtn:nexaction}
	 		\State $(Pmax', Pmin', ev') = \perp, \perp, -\infty$
			\ForAll{$plano \in getPlanosMin(estadoDoJogo)$} \label{alg:meuahtn:for}
		 		\State $(Pmax, Pmin, ev) = \Call{AHTNMin}{(estado, planoMax, planoMin, deph-1)}$ \label{alg:meuahtn:ahtnmin}
			 	\If {$ev' > ev$} \label{alg:meuahtn:avali}
					\State $(Pmax', Pmin', ev') = (Pmax, Pmin, ev)$
				\EndIf		 		 		
		 	\EndFor
		 	\State \Return $(Pmax', Pmin', ev')$ \label{alg:meuahtn:retorno}
 		\EndFunction
 	\end{algorithmic}
 \end{algorithm}
 
 
O método $\mathit{getAction}$ é responsável pela chamada do algoritmo de AHTN. 
Ele realiza a chamada para o método de $\mathit{AHTNMax}$ para todos os planos possíveis no estado atual.
O retorno indica qual plano tem a melhor função de avaliação.
A primeira ação do plano com a melhor função de avaliação é a escolhida para ser executada. 

O método $\mathit{getAction}$ é chamado a todo o ciclo para decidir qual a jogada deve ser realizada.
As ações geralmente não são realizadas em um clico.
Por exemplo, treinar uma unidade leva alguns ciclos.
Assim o algoritmo gera muitas vezes a mesma ação.
Foi definido um tempo de 50 ciclos para gerar uma nova ação.
Com isso o número de ações geradas repetidamente foi reduzido.

